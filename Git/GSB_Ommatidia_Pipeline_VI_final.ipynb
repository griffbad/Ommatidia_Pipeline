{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compound Eye Ommatidia Analysis\n",
    "This notebook will grab data from either HDF5 or a TIFF stack, and find the diameter of all the ommatidia, as well as interommatidial angle between all nearest neighbors. IT WILL DO THIS FOR ALL LABELED DATA. This means bad cones, artifacts, anything. In theory I could add some data cleaning stuff, but ideally that is all done before using this. I have mainly tested this on chunks of data (instead of an entire eye), with the goal of combining the data in post. It _should_ work for full eye sets, but I have not tested. Email (griffinb@uchicago.edu) or slack me or whatever if you have questions, suggestions, or want to see code for this being run on one cone with PCA and process visualization - it's quite cool. \n",
    "\n",
    "This also has a simple process for saving output so input/PCA only has to be run once on a dataset, and then reloading the output of that is almost instantaneous. It also can take data from the output of Prof. Palmer's MATLAB code. \n",
    "\n",
    "I also use a stupid number of Jupyter Lab extensions for this. If you have issues let me know and I can make/send a non-extension one (which will break my heart). \n",
    "\n",
    "\n",
    "#### Other assumptions/notes:\n",
    "- Background is zero\n",
    "- Most units have been converted to micrometers (which really only means diameters)\n",
    "- Because I'm dumb, much of this code is intended to be run in order! \n",
    "\n",
    "#### #To-Do:\n",
    "- Circles around highest volume area?\n",
    "- Implement [COO](https://sparse.pydata.org/en/stable/#)\n",
    "\n",
    "#### Full list of Jupyter Lab extensions I used:\n",
    "- [jupyter-matplotlib](https://github.com/matplotlib/ipympl)\n",
    "- [jupyterlab-manager](https://github.com/jupyter-widgets/ipywidgets)\n",
    "\n",
    "#### Trouble shooting\n",
    "- If Imports fail, remove the last line\n",
    "- If you're getting errors about tqdm, remove it (e.g. go from notebook.tqdm(\\_\\_\\_\\_\\_\\_) to just \\_\\_\\_\\_\\_\\_). This will not remove any functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Imports\n",
    "If the import cell below fails, try commenting out the last line (%matplotlib qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.transform import rescale,resize\n",
    "from skimage import morphology\n",
    "import math \n",
    "import os \n",
    "from PIL import Image\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pylab import *\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "import json\n",
    "import csv\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Let's get our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the resolution (nm^3 per voxel) of your dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_to_nm = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: New dataset\n",
    "Run this if you have a dataset that you have not run this code with before. \n",
    "\n",
    "It should be either an HDF5 or a TIFF sequence. If you want other implementations, let me know, it's super easy.\n",
    "\n",
    "**Note:** This may take a while (2+ hours on some large datasets). If you have any idea how to make this more efficient, please let me know I would love to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write RELATIVE location + title of HDF5 (e.g. /Griffin/Cones/ygw15.h5) or location of TIFF stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = './DONE_HDFS/ygw34/ygw34_done.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_name[-3:-1]=='.h':\n",
    "    f = h5py.File(f_name, 'r')\n",
    "    data = np.asarray(f['label'], dtype='uint16')\n",
    "    f.close()\n",
    "else:\n",
    "    data = []\n",
    "    for fname in notebook.tqdm(os.listdir(f_name)):\n",
    "        im = Image.open(os.path.join(f_name, fname))\n",
    "        imarray = np.array(im)\n",
    "        data.append(imarray)\n",
    "\n",
    "if(np.max(data)==1):\n",
    "    data=morphology.label(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a0e2c89f204c74899ce982979126e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=np.asarray(data)\n",
    "pc_dict = defaultdict(list)\n",
    "shape = data.shape\n",
    "for x in notebook.tqdm(np.arange(shape[0])):\n",
    "    for y in np.arange(shape[1]):\n",
    "        for z in np.arange(shape[2]): \n",
    "            if data[x,y,z]!=0:\n",
    "                pc_dict[data[x,y,z]].append([x,y,z]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of full dataset: (800, 868, 828)\n",
      "Number of unique labels: 908\n"
     ]
    }
   ],
   "source": [
    "orig_data_size = data.shape\n",
    "print(\"Shape of full dataset: \" + str(orig_data_size))\n",
    "\n",
    "print(\"Number of unique labels: \" + str(len(pc_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Pre-run data\n",
    "If you have already run this code on a dataset, and have the Pickle's from that run stored somewhere, you can use this!\n",
    "\n",
    "This is currently set up to pull stuff out of a Pickle that stores EVERYTHING from a PCA run, and from there you can ignore stuff if you want to. Code to convert a post-PCA run of data is at the bottom of Part 2 of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    pc_dict.items()\n",
    "    raise NameError('WARNING: YOU SEEM TO HAVE ALREADY RAN A NEW DATASET THROUGH. RUNNING THIS NEXT SECTION (OPTION B) MAY OVERWRITE.')\n",
    "except:\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fname = './PRERUN_DATA/ygw33_all_01.pickle'\n",
    "with open(all_fname, 'rb') as handle:\n",
    "    [pc_dict, all_diams, all_PCAs, all_centroids, all_first_PCs, pc_norm_dict] = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option C: A .mat file output of Prof. Palmer's code\n",
    "\n",
    "This assumes the output is in exactly the format that was sent to me, which seems kind of likely tbh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "WARNING: YOU SEEM TO HAVE ALREADY IMPORTED FROM A PRE-PICKLED DATA SET. RUNNING THE NEXT BLOCK (OPTION C) MAY OVERWRITE YOUR DATA",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-abb351e1e01f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mall_PCAs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'WARNING: YOU SEEM TO HAVE ALREADY IMPORTED FROM A PRE-PICKLED DATA SET. RUNNING THE NEXT BLOCK (OPTION C) MAY OVERWRITE YOUR DATA'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: WARNING: YOU SEEM TO HAVE ALREADY IMPORTED FROM A PRE-PICKLED DATA SET. RUNNING THE NEXT BLOCK (OPTION C) MAY OVERWRITE YOUR DATA"
     ]
    }
   ],
   "source": [
    "if all_PCAs:\n",
    "    raise NameError('WARNING: YOU SEEM TO HAVE ALREADY IMPORTED FROM A PRE-PICKLED DATA SET. RUNNING THE NEXT BLOCK (OPTION C) MAY OVERWRITE YOUR DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = './ygw32cc_01_tester/from_matlab/ygw32cc_01labels_1.mat'\n",
    "all_mat = sio.loadmat(f_name)\n",
    "\n",
    "pc_dict = defaultdict(list)\n",
    "\n",
    "x_pc = all_mat['labels_xvals']\n",
    "y_pc = all_mat['labels_yvals']\n",
    "z_pc = all_mat['labels_zvals']\n",
    "\n",
    "for i in np.arange(1, x_pc.shape[1]+1):\n",
    "    total = np.transpose([x_pc[0,i-1], y_pc[0,i-1], z_pc[0,i-1]])[0,:,:]\n",
    "    pc_dict[i] = total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: PCA and Diameters\n",
    "\n",
    "## *No need to run this if you used option B from above and have not made any changes to the PCA analysis done here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if all_PCAs:\n",
    "    throw### Tell me here what 1 pixel is equal to in nanometers.\n",
    "(e.g. if 1 pixel = 600 nm^3, write 600 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_PCAs:\n",
    "    raise NameError('WARNING: YOU ARE READING FROM A PRE-RUN DATA COLLECTION. There is likely no need to run the next block of code, which does PCA.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736e04b2b7924ad6ad8d4cdd96881db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_PCAs = defaultdict(list)\n",
    "all_diams = defaultdict(list)\n",
    "all_centroids = defaultdict(list)\n",
    "all_first_PCs = defaultdict(list)\n",
    "bad_label = []\n",
    "all_norm_locs = defaultdict(list)\n",
    "pc_norm_dict = defaultdict(list)\n",
    "global_pointcloud = defaultdict(list)\n",
    "sample_width = 1\n",
    "# pragma omp parallel for\n",
    "for item in notebook.tqdm(pc_dict.keys()):\n",
    "    point_cloud = []\n",
    "    test_coord = pc_dict[item]                               # Gets all the locations for the current label\n",
    "    norm_coord = test_coord - np.mean(test_coord, axis=0)    # Normalizes all the coordinates so the centroid is at 0,0,0. Key step for this PCA hack\n",
    "    \n",
    "    try:\n",
    "        pc_norm_dict[item] = (norm_coord) \n",
    "        sparse_norm_coord = norm_coord[::sample_width,:]     # This samples every sample_width locations. This makes this process IMPERFECT but FASTER. Lower sample_width will be more precise, higher will be faster\n",
    "        pca = PCA(n_components=3)                            # Don't do less than 3, and there's really no reason to do more than 3        \n",
    "        pca.fit(sparse_norm_coord)\n",
    "        \n",
    "        [uall,vall,wall]=pca.components_\n",
    "        \n",
    "        for i in norm_coord:\n",
    "            i = i - np.dot(uall,i)*uall\n",
    "            point_cloud.append(i) \n",
    "      \n",
    "    \n",
    "        flat_rep = []\n",
    "        \n",
    "        for i in point_cloud:\n",
    "            (x,y) = (np.dot(vall,i), np.dot(wall,i))        # Honestly don't remember what this does. Wrote it in a fevered rush inspired by Philz's Tesora blend.\n",
    "            flat_rep.append((x,y))\n",
    "            \n",
    "        flat_rep=np.asarray(flat_rep)\n",
    "       \n",
    "        dist_list = np.sort(list(\n",
    "            map(lambda x: \n",
    "            np.linalg.norm((0,0) - x) , flat_rep)))         # Determines distance for all points in flattened cone)\n",
    "        \n",
    "        ## PAST HERE IS A HEURISTIC FOR DETERMINING A REASONABLE RADIUS INTERPRETATION. ##\n",
    "        ## The math is basically that 2/(.5*r) of the points in a circle are along the circumference,\n",
    "        ## so if I take the sorted list of distances from center, take that many from the high end, \n",
    "        ## and average those values, I should get a pretty good estimate of the diameter of the circle.\n",
    "        longest_r = 2 * np.max(dist_list)\n",
    "        ratio = 2/(.5*longest_r)\n",
    "        num_to_keep = math.floor(ratio*len(dist_list))         # Number of points used to estimate radius.   \n",
    "        edge_locs = dist_list[len(dist_list)-num_to_keep:]\n",
    "        \n",
    "        if(2*np.average(edge_locs))>10:\n",
    "            ## And below here I just save some of these results to dictionaries\n",
    "            all_diams[item].append(px_to_nm*2*np.average(edge_locs))\n",
    "            all_PCAs[item].append(pca.components_)\n",
    "            all_centroids[item].append(np.mean(test_coord, axis=0))\n",
    "            all_first_PCs[item].append(uall)\n",
    "            all_norm_locs[item].append(norm_coord[:])\n",
    "            test_coord = np.asarray(test_coord)\n",
    "            global_pointcloud[item].append(test_coord)\n",
    "        else:\n",
    "#             print(\"This is too small: \" + str(item))\n",
    "            bad_label.append(item)\n",
    "                \n",
    "    ## This will usually happen if there are less then 2*sample_width(?) points in the label. Helpful to get rid of artifacts and shouldn't happen to pristine data. ##\n",
    "    except:\n",
    "#         print(\"BREAKS ON THIS LABEL: \" + str(item))\n",
    "        bad_label.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code will save EVERYTHING from PCA above into a single [Pickle file](https://docs.python.org/3/library/pickle.html)! Please run so life is enjoyable!\n",
    "\n",
    "Also very useful if you want to upload and combine multiple datasets, but I'm not going to implement that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './PRERUN_DATA/' \n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "all_fname = '/ygw34_all_02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+all_fname+'.pickle', 'wb') as handle:\n",
    "    pickle.dump([pc_dict, all_diams, all_PCAs,\n",
    "                 all_centroids, all_first_PCs, pc_norm_dict], \n",
    "                 handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Let's look at some *raw* data!\n",
    "\n",
    "You will notice that I got kind of bored here and started messing around with some other stastics options that may be interesting to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "[limx, limy, limz] = [np.max(np.asarray(list(all_centroids.values()))[:,:,0]),\n",
    "                      np.max(np.asarray(list(all_centroids.values()))[:,:,1]),\n",
    "                      np.max(np.asarray(list(all_centroids.values()))[:,:,2])]\n",
    "\n",
    "limtot = np.ceil(np.max((limx,limy,limz)))\n",
    "\n",
    "## If you want every plot to be equal on every axis, uncomment below this line ##\n",
    "\n",
    "# limx = limtot\n",
    "# limy = limtot\n",
    "# limz = limtot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we really begin, a sanity check: all the centroids in 3D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x23fc35ac608>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "all_XS = np.asarray(list(all_centroids.values()))[:,:,0] \n",
    "all_YS = np.asarray(list(all_centroids.values()))[:,:,1]\n",
    "all_ZS = np.asarray(list(all_centroids.values()))[:,:,2]\n",
    "ax.set_title('Centroids in 3D Space')\n",
    "ax.set_xlim(0,limx)\n",
    "ax.set_ylim(0,limy)\n",
    "ax.set_zlim(0,limz)\n",
    "ax.scatter(all_XS,all_YS,all_ZS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, a histogram of _all_ the diameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average diameter: 8853.991602766913\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "ax.hist(np.asarray(list(all_diams.values())), bins=200)\n",
    "ax.set_title('Histogram of all diameters (nm)')\n",
    "\n",
    "all_median_diam = np.median(list(all_diams.values()))\n",
    "print('Average diameter: ' + str(all_median_diam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we look at how the diameters differ across the specimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(869, 1)\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "diams_arr = np.asarray(list(all_diams.values()))[:,0]\n",
    "colors = cm.jet(diams_arr/np.max(diams_arr))\n",
    "colmap = cm.ScalarMappable(cmap=cm.jet)\n",
    "colmap.set_array(diams_arr)\n",
    "cb = fig.colorbar(colmap)\n",
    "\n",
    "ax.set_title('Centroids in 3D Space Heatmapped to Diameter')\n",
    "ax.scatter(all_XS,all_YS,all_ZS,\n",
    "           c=colors)\n",
    "fig.show()\n",
    "print(all_XS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have a 3D plot of all of the first PCs in 3D space\n",
    "\n",
    "(e.g. a PC of <.5,.2,.3> will be plotted in 3D at (.5, .2, .3))\n",
    "\n",
    "This isn't really useful, but it's a good sanity check. For a full data set, this should look like either a full sphere or a half sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x240c64bb708>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "all_PCAs_arr = np.asarray(list(all_PCAs.values()))\n",
    "all_first_PCs_arr = np.asarray(list(all_first_PCs.values()))\n",
    "\n",
    "all_firstP_XS = all_first_PCs_arr[:,:,0]\n",
    "all_firstP_YS = all_first_PCs_arr[:,:,1]\n",
    "all_firstP_ZS = all_first_PCs_arr[:,:,2]\n",
    "ax.set_title('First PCs in 3D Space')\n",
    "ax.scatter(all_firstP_XS,all_firstP_YS,all_firstP_ZS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we plot centroids with their attached PCs\n",
    "\n",
    "Also a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Line3DCollection at 0x2405a75bd48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_firstP_XS = all_first_PCs_arr[:,:,1]\n",
    "all_firstP_YS = all_first_PCs_arr[:,:,1]\n",
    "all_firstP_ZS = all_first_PCs_arr[:,:,2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_xlim(0, limtot)\n",
    "ax.set_ylim(0, limtot)\n",
    "ax.set_zlim(0, limtot)\n",
    "ax.scatter(all_XS, all_YS, all_ZS, alpha=1, linewidth=3)\n",
    "ax.set_title('Centroids in 3D Space w/ attached first PCs')\n",
    "ax.quiver(all_XS, all_YS, all_ZS, all_firstP_XS*1000, all_firstP_YS*1000, all_firstP_ZS*1000, color='red', alpha=.2, linewidth=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Line3DCollection at 0x24057e8f0c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_2P_XS = all_PCAs_arr[:,:,0,0]\n",
    "all_2P_YS = all_PCAs_arr[:,:,0,1]\n",
    "all_2P_ZS = all_PCAs_arr[:,:,0,2]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_xlim(0, limtot)\n",
    "ax.set_ylim(0, limtot)\n",
    "ax.set_zlim(0, limtot)\n",
    "ax.scatter(all_XS, all_YS, all_ZS, alpha=1, linewidth=3)\n",
    "ax.set_title('Centroids in 3D Space w/ attached first PCs')\n",
    "ax.quiver(all_XS, all_YS, all_ZS, all_2P_XS*600, all_2P_YS*600, all_2P_ZS*600, color='red', alpha=.2, linewidth=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.2: Remove Bad Data (based on diameter)\n",
    "\n",
    "The most consistent global property of the ommatidia is their diameter, of which they are incredibly consistent. This next chunk of code will remove any data that has a diameter that is more than 1.25x or less than .75x the median diameter of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 111 bad data.\n"
     ]
    }
   ],
   "source": [
    "bad_vals = []\n",
    "\n",
    "diam_std = np.std(np.asarray(list(all_diams.values())))\n",
    "bottom = all_median_diam - diam_std\n",
    "top = all_median_diam + diam_std\n",
    "\n",
    "\n",
    "for i in all_diams.keys():\n",
    "    if all_diams.get(i)<bottom or all_diams.get(i)>top:\n",
    "        bad_vals.append(i)\n",
    "        \n",
    "bad_vals=np.asarray(bad_vals)\n",
    "print(\"There were \" + str(bad_vals.shape[0]) + \" bad data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show the centroids of those bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(908,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x240c4769bc8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# fig2 = plt.figure()\n",
    "# ax2 = fig2.gca(projection='3d')\n",
    "good_centroids_dict = all_centroids.copy()\n",
    "good_diams_dict = all_diams.copy()\n",
    "good_PCAs_dict = all_PCAs.copy()\n",
    "good_PC_dict = pc_dict.copy()\n",
    "print(np.asarray(list(pc_dict.keys())).shape)\n",
    "good_first_PCs_dict = all_first_PCs.copy()\n",
    "good_pc_norm_dict = pc_norm_dict.copy()\n",
    "\n",
    "bad_centroids = np.asarray([good_centroids_dict[x] for x in bad_vals])\n",
    "bad_XS = bad_centroids[:,:,0]\n",
    "bad_YS = bad_centroids[:,:,1]\n",
    "bad_ZS = bad_centroids[:,:,2]\n",
    "\n",
    "for i in bad_vals:\n",
    "    [curr_x, curr_y, curr_z] = np.asarray(good_centroids_dict.pop(i))[0]\n",
    "    curr_diam = np.asarray(good_diams_dict.pop(i))\n",
    "    good_PCAs_dict.pop(i)\n",
    "    good_PC_dict.pop(i)\n",
    "    good_first_PCs_dict.pop(i)\n",
    "    good_pc_norm_dict.pop(i)\n",
    "\n",
    "ax.set_title('Centroids of Bad Data in 3D')\n",
    "ax.scatter(bad_XS,bad_YS,bad_ZS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758\n",
      "797\n"
     ]
    }
   ],
   "source": [
    "path = './PRERUN_DATA/' \n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "print(len(list(good_centroids_dict.values())))\n",
    "all_fname = '/ygw18_volumes.pickle'\n",
    "good_vols = [len(x) for x in list(good_PC_dict.values())]\n",
    "print(len(good_vols))\n",
    "with open(path+all_fname, 'wb') as handle:\n",
    "    pickle.dump(good_vols, \n",
    "                 handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_vols_arr = np.asarray(good_vols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './PRERUN_DATA/' \n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "all_fname = '/ygw33_good_data2.pickle'\n",
    "\n",
    "with open(path+all_fname, 'wb') as handle:\n",
    "    pickle.dump([good_PC_dict, good_diams_dict, good_PCAs_dict,\n",
    "                 good_centroids_dict, good_first_PCs_dict, good_pc_norm_dict], \n",
    "                 handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Interommatidial angle (and distance) between nearest neighbors\n",
    "\n",
    "In which I abuse sklearn.NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_PCAs_arr = np.asarray(list(good_PCAs_dict.values()))[:]\n",
    "good_centroids_arr =  np.asarray(list(good_centroids_dict.values()))[:]\n",
    "good_diams_arr =  np.asarray(list(good_diams_dict.values()))[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors = 2, \n",
    "                        algorithm = 'brute', metric='euclidean', \n",
    "                        leaf_size=60, p=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "nbrs_f=nbrs.fit(good_centroids_arr[:,0,:])\n",
    "dists, indecs = nbrs_f.kneighbors(good_centroids_arr[:,0,:])\n",
    "dists=dists*px_to_nm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we look at a histogram of nearest neighbor interommatidial distances.\n",
    "\n",
    "Uhhh... if someone knows how to use matplotlib better than me and can zoom in on this before loading the graph, this data is kinda cool but not super useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. distance between nearest neighbors: 11139.28527116133\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "x_range = np.arange(0,np.max(dists),.05)\n",
    "ax.hist(dists[:,1], bins=100)\n",
    "ax.set_title('Histogram of Interommatidial Distance Between Nearest Neighbors (nm)')\n",
    "fig.show()\n",
    "print('Avg. distance between nearest neighbors: ' + str(np.median(dists[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we look at the diameter graph, but this time of GOOD diameters, again because it's super pertinent info right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of GOOD diameters (nm)')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "ax.hist(good_diams_arr, bins=200)\n",
    "ax.set_title('Histogram of GOOD diameters (nm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This plots all the centroids and connects the nearest neighbors. This was a really important sanity check honestly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = list(good_centroids_dict.keys())\n",
    "\n",
    "key_indecs = []\n",
    "for i in indecs:\n",
    "    k1 = all_keys[i[0]]\n",
    "    k2 = all_keys[i[1]]\n",
    "    key_indecs.append((k1,k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 820.0340471607315)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GXS = good_centroids_arr[:,0,0]\n",
    "GYS = good_centroids_arr[:,0,1]\n",
    "GZS = good_centroids_arr[:,0,2]\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.set_title('Connected Neighbors in 3D')\n",
    "ax.scatter(GXS, GYS, GZS)\n",
    "for i in key_indecs:\n",
    "    p1 = good_centroids_dict[i[0]][0]\n",
    "    p2 = good_centroids_dict[i[1]][0]\n",
    "    ax.plot((p1[0],p2[0]), (p1[1], p2[1]), (p1[2],p2[2]))\n",
    "    \n",
    "lim = np.max([limx, limy, limz])\n",
    "ax.set_xlim(0, lim)\n",
    "ax.set_ylim(0, lim)\n",
    "ax.set_zlim(0, lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we calculate interommatidial angle.\n",
    "\n",
    "Here, I treat the first PC as the angle of the cone. I am equivalently confident that this is appropriate as I am that the PCA hack works above for finding diameter (this is because they are equivalent problems, I think). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. interommatidial angle: 3.4991269049106686\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "all_angles = defaultdict(float)\n",
    "for i in key_indecs:\n",
    "    PC1 = np.asarray(all_first_PCs.get(i[0]))[0,:]\n",
    "    PC2 = np.asarray(all_first_PCs.get(i[1]))[0,:]\n",
    "    angle = np.arccos(\n",
    "        np.dot(PC1,PC2) /\n",
    "        (np.linalg.norm(PC1)*np.linalg.norm(PC2)))\n",
    "#   all_angles[i[0]] = angle\n",
    "    all_angles[i[0]] = math.degrees(angle)%90\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "ax.hist(all_angles.values(), bins=100)\n",
    "ax.set_title('Histogram of Interommatidial Angle Between Nearest Neighbors (Phi) (Degrees)')\n",
    "fig.show()\n",
    "print('Avg. interommatidial angle: ' + str(np.median(np.asarray(list(all_angles.values())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "angle_arr = np.asarray(list(all_angles.values()))\n",
    "colors = cm.jet(angle_arr/np.max(angle_arr))\n",
    "colmap = cm.ScalarMappable(cmap=cm.jet)\n",
    "colmap.set_array(angle_arr)\n",
    "cb = fig.colorbar(colmap)\n",
    "\n",
    "ax.set_title('Interommatidial Angles in 3D Space')\n",
    "ax.scatter(GXS,GYS,GZS,\n",
    "           c=colors)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(797,)\n",
      "(758,)\n"
     ]
    }
   ],
   "source": [
    "print(good_vols_arr.shape)\n",
    "print(GXS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'c' argument has 797 elements, which is inconsistent with 'x' and 'y' with size 758.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-71db4502efa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Volume of Ommatidia in 3D Space'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m ax.scatter(GXS,GYS,GZS,\n\u001b[1;32m---> 12\u001b[1;33m            c=colors)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\griff\\anaconda3\\lib\\site-packages\\mpl_toolkits\\mplot3d\\axes3d.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, xs, ys, zs, zdir, s, c, depthshade, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2310\u001b[0m             \u001b[0mzs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2312\u001b[1;33m         \u001b[0mpatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2313\u001b[0m         art3d.patch_collection_2d_to_3d(patches, zs=zs, zdir=zdir,\n\u001b[0;32m   2314\u001b[0m                                         depthshade=depthshade)\n",
      "\u001b[1;32mD:\\Users\\griff\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\griff\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m                          \u001b[1;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 **kwargs)\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\griff\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4451\u001b[0m             self._parse_scatter_color_args(\n\u001b[0;32m   4452\u001b[0m                 \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4453\u001b[1;33m                 get_next_color_func=self._get_patches_for_fill.get_next_color)\n\u001b[0m\u001b[0;32m   4454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4455\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mplotnonfinite\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolors\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\griff\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[1;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[0;32m   4305\u001b[0m                     \u001b[1;31m# NB: remember that a single color is also acceptable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4306\u001b[0m                     \u001b[1;31m# Besides *colors* will be an empty array if c == 'none'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4307\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0minvalid_shape_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4308\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4309\u001b[0m             \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# use cmap, norm after collection is created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'c' argument has 797 elements, which is inconsistent with 'x' and 'y' with size 758."
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "vols_arr = np.asarray(good_vols)\n",
    "colors = cm.jet(vols_arr/np.max(vols_arr))\n",
    "colmap = cm.ScalarMappable(cmap=cm.jet)\n",
    "colmap.set_array(vols_arr)\n",
    "cb = fig.colorbar(colmap)\n",
    "\n",
    "ax.set_title('Volume of Ommatidia in 3D Space')\n",
    "ax.scatter(GXS,GYS,GZS,\n",
    "           c=colors)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4.2: BARLOW RATIO\n",
    "\n",
    "This is an attempt to recreate exactly the ratio Barlow wanted (Φ : θ = interommatidial angle / max resolving angle).\n",
    "\n",
    "I assume wavelength = 500 nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. resolution angle (deg): 4.005366218790115\n"
     ]
    }
   ],
   "source": [
    "all_res = []\n",
    "all_barlow_ratios = []\n",
    "\n",
    "for i in key_indecs:\n",
    "    diam = good_diams_dict.get(i[0])[0]\n",
    "    rayleigh = 1.22 * (5*pow(10,-5)) / (diam  * pow(10, -7)) ##CHECK HERE\n",
    "    rayleigh = math.degrees(rayleigh)   \n",
    "    barlow_ratio = all_angles.get(i[1])/rayleigh\n",
    "    all_res.append(rayleigh)\n",
    "    all_barlow_ratios.append(barlow_ratio)\n",
    "\n",
    "all_res = np.asarray(all_res)\n",
    "all_barlow_ratios = np.asarray(all_barlow_ratios)\n",
    "print('Avg. resolution angle (deg): ' + str(np.median(all_res)))\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "x_range = np.arange(0,np.max(dists),.05)\n",
    "ax.hist(all_res, bins=200)\n",
    "ax.set_title('Histogram of Theta (Degrees)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "angle_arr = np.asarray(list(all_angles.values()))\n",
    "colors = cm.jet(all_res/np.max(all_res))\n",
    "colmap = cm.ScalarMappable(cmap=cm.jet)\n",
    "colmap.set_array(all_res)\n",
    "cb = fig.colorbar(colmap)\n",
    "\n",
    "ax.set_title('Rayleigh Angles in 3D Space')\n",
    "ax.scatter(GXS,GYS,GZS,\n",
    "           c=colors)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Barlow Ratio: 0.9147524864949292\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "x_range = np.arange(0,np.max(dists),.05)\n",
    "ax.hist(all_barlow_ratios, bins=200)\n",
    "ax.set_title('All Barlow Ratios')\n",
    "fig.show()\n",
    "print(\"Average Barlow Ratio: \"+str(np.median(all_barlow_ratios)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "barlows_arr = np.asarray(all_barlow_ratios)\n",
    "colors = cm.jet(barlows_arr/100)\n",
    "colmap = cm.ScalarMappable(cmap=cm.jet)\n",
    "colmap.set_array(barlows_arr)\n",
    "cb = fig.colorbar(colmap)\n",
    "\n",
    "ax.set_title('Barlow Ratios in 3D Space')\n",
    "ax.scatter(GXS,GYS,GZS,\n",
    "           c=colors)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will save all of the import data to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\griff\\\\Box\\\\College\\\\Honors\\\\BERTT'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./DONE_HDFS/ygw33/all_angles2.csv\", np.asarray(list(all_angles.values())), delimiter = ',')\n",
    "np.savetxt(\"./DONE_HDFS/ygw33/all_rayleighs2.csv\", all_res, delimiter=',')\n",
    "np.savetxt(\"./DONE_HDFS/ygw33/all_barlows2.csv\", all_barlow_ratios, delimiter=',')\n",
    "np.savetxt(\"./DONE_HDFS/ygw33/all_diams2.csv\", np.asarray(list(good_diams_dict.values())), delimiter=',')\n",
    "np.savetxt(\"./DONE_HDFS/ygw33/all_dists2.csv\", dists[:,1], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Average Cone\n",
    "The argument here is that if we are to simulate light through the cones, the best way to do it is not with one or two selected cones recreated by hand, but a platonic cone created by a sort of mode-like analysis of the other cones. I here take normalized (meaning the centroid is at 0,0,0) point cloud for each sample, rotate them so they are facing the same direction, then heatmap the different points (by heatmap I mean attach to each point a number value saying how often that point shows up in all of the normalized point clouds). This then plots the points that are above a certain occurrence value and saves the pointcloud above that threshold into a 3D HDF5. Also has saving and loading functionality b/c the process of heatmapping all the points can take a while (in theory I may try to speed this up but you also should only be running it once per dataset so like... eh?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.gca(projection='3d')\n",
    "full_norm_dict = defaultdict(list)\n",
    "full_norm_list = []\n",
    "for i in good_pc_norm_dict.keys():\n",
    "# for i in np.arange(600,610):\n",
    "   \n",
    "    all_vecs = np.asarray(good_PCAs_dict[i])\n",
    "    if np.size(all_vecs) > 1:\n",
    "\n",
    "        og_pc = np.transpose(np.asarray(good_pc_norm_dict[i]))\n",
    "        inv_PCs =np.linalg.inv(np.transpose(all_vecs[0,:,:]))\n",
    "        new_pc = np.floor(np.matmul(inv_PCs,og_pc))\n",
    "        full_norm_dict[i]=np.transpose(new_pc)\n",
    "        np.append(full_norm_list, np.transpose(new_pc))\n",
    "#         fig = plt.figure()\n",
    "#         ax = fig.gca(projection='3d')\n",
    "#         ax.set_xlim(-25,25)\n",
    "#         ax.set_ylim(-25,25)\n",
    "#         ax.set_zlim(-25,25)\n",
    "#         ax.scatter(new_pc[0,:], new_pc[1,:], new_pc[2,:])\n",
    "        \n",
    "full_norm_list=np.asarray(full_norm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e663f5e57b0d4a878237c92ac4bd8830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1061 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_range = [np.min(new_pc[:,0]), np.max(new_pc[:,0])]\n",
    "y_range = [np.min(new_pc[:,1]), np.max(new_pc[:,1])]\n",
    "z_range = [np.min(new_pc[:,2]), np.max(new_pc[:,2])]\n",
    "\n",
    "loc_heat_map = defaultdict(uint16)\n",
    "                \n",
    "for i in notebook.tqdm(full_norm_dict.keys()):\n",
    "    comp = full_norm_dict[i]\n",
    "    for loc in comp:\n",
    "        loc_heat_map[str(loc)]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lochm_fname = './DONE_HDFS/ygw18/heatmap_all.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(lochm_fname, 'wb') as handle:\n",
    "    pickle.dump(loc_heat_map,handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54383\n"
     ]
    }
   ],
   "source": [
    "with open(lochm_fname, 'rb') as handle:\n",
    "    loc_heat_map=pickle.load(handle)\n",
    "    \n",
    "print(len(loc_heat_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100106,)\n",
      "(10982, 3)\n"
     ]
    }
   ],
   "source": [
    "n_loc_heat = loc_heat_map.copy()\n",
    "plot_locs = []\n",
    "all_vals = np.asarray(list(n_loc_heat.values()))\n",
    "print(all_vals.shape)\n",
    "coloro = 0\n",
    "limit = 250\n",
    "\n",
    "for i in loc_heat_map.keys():\n",
    "    if all_vals[coloro]>limit:\n",
    "        chunks = i[1:-1].strip().split('.')\n",
    "        try:\n",
    "            plot_locs.append([int(chunks[0]),int(chunks[1]),int(chunks[2])])\n",
    "        except:\n",
    "            plot_locs.append([0,0,0])\n",
    "    else:\n",
    "        n_loc_heat.pop(i)\n",
    "        all_vals[coloro]=0\n",
    "    coloro+=1\n",
    "\n",
    "maxxer = np.max(all_vals)\n",
    "    \n",
    "all_vals = np.asarray(list(n_loc_heat.values()))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "colors = cm.jet(all_vals/np.max(all_vals))\n",
    "colmap = cm.ScalarMappable(cmap=cm.jet)\n",
    "colmap.set_array(all_vals)\n",
    "cb = fig.colorbar(colmap)\n",
    "ax.set_xlim(-25,25)\n",
    "ax.set_ylim(-25,25)\n",
    "ax.set_zlim(-25,25)\n",
    "plot_locs=np.asarray(plot_locs)\n",
    "ax.set_title('Points that occur in '+str(math.floor(100*(1-(limit/maxxer)))) +'% of the objects')\n",
    "ax.scatter(plot_locs[:,0], plot_locs[:,1], plot_locs[:,2], c=colors)\n",
    "print(plot_locs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16189, 3)\n"
     ]
    }
   ],
   "source": [
    "x_min = np.min(plot_locs[:,0])\n",
    "y_min = np.min(plot_locs[:,1])\n",
    "z_min = np.min(plot_locs[:,2])\n",
    "\n",
    "trans_locs = plot_locs\n",
    "\n",
    "trans_locs[:,0]-=x_min\n",
    "trans_locs[:,1]-=y_min\n",
    "trans_locs[:,2]-=z_min\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_xlim(0,50)\n",
    "ax.set_ylim(0,50)\n",
    "ax.set_zlim(0,50)\n",
    "ax.set_title('Points that occur '+str(limit)+' times or more')\n",
    "ax.scatter(trans_locs[:,0], trans_locs[:,1], trans_locs[:,2])\n",
    "print(plot_locs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 31, 22)\n",
      "[]\n",
      "(3, 16111)\n"
     ]
    }
   ],
   "source": [
    "x_max = np.max(trans_locs[:,0])\n",
    "y_max = np.max(trans_locs[:,1])\n",
    "z_max = np.max(trans_locs[:,2])\n",
    "\n",
    "platonic_cone_arr = np.zeros((x_max, y_max, z_max))\n",
    "print(platonic_cone_arr.shape)\n",
    "print(np.asarray(np.where(platonic_cone_arr==255)))\n",
    "# for [locx,locy,locz] in trans_locs[:,:]:\n",
    "for i in np.arange(trans_locs.shape[0]):\n",
    "    [locx,locy,locz] = trans_locs[i,:].astype(int)-1\n",
    "    platonic_cone_arr[locx,locy,locz]=255\n",
    "print(np.asarray(np.where(platonic_cone_arr==255)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cone_f = h5py.File('./DONE_HDFS/ygw18/platonic_cone.h5', 'w')\n",
    "d = cone_f.create_dataset('cone', data=platonic_cone_arr)\n",
    "cone_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
